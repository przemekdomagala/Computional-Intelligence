{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53200e55-70d1-4325-813a-e94a0d9dc6dd",
   "metadata": {},
   "source": [
    "Install Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9bac1-e8a0-4fbb-8eeb-2ea170918c08",
   "metadata": {},
   "source": [
    "Import the prerequisite packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c158a68-ce89-4e36-87b1-b90e78c96978",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2882,
    "lastExecutedAt": 1733643247641,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.distributions as distributions\nimport numpy as np\nimport gymnasium as gym  \n"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "import numpy as np\n",
    "import gymnasium as gym  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e6079-f39a-4148-aad5-ac9c7e482964",
   "metadata": {},
   "source": [
    "Check the available environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966e177-dd69-4d22-a56f-4c93b0c9548c",
   "metadata": {},
   "source": [
    "Create a new CartPole-v1 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03adab6-6e63-4a23-a45f-3559294480c7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1733643247745,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import gymnasium as gym\nenv = gym.make('CartPole-v1')\n"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4cdade-271e-4c7b-817a-9a87a3a1ce25",
   "metadata": {},
   "source": [
    "Check the observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "351c761d-11e6-4d21-a4a3-f3b7f7304278",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1733643247793,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(\"observation space: \", env.observation_space)",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space:  Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation space: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca42f96-eb75-4ffb-80de-1acb3d7f80a4",
   "metadata": {},
   "source": [
    "Check an example of an observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "002c2d79-a810-4b80-a2d9-9cc5e5cbc86f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1733643247841,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "observation, info = env.reset()\nprint(\"observation: \", observation)\n",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:  [-0.01170683 -0.04804509  0.04595344 -0.0026174 ]\n"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "print(\"observation: \", observation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c577b8-eb84-4875-8b71-93b8f7b26e40",
   "metadata": {},
   "source": [
    "Check the action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "968d95c1-d3fe-4768-880a-760301b974f3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1733643247893,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(\"action space: \", env.action_space)",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space:  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(\"action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4d8f1a-0e51-40d7-bfa8-1f866a0a6d28",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1733643247941,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "env = gym.make('CartPole-v1')"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e9ecb-ec9e-4554-bfe5-e027d781b88a",
   "metadata": {},
   "source": [
    "Choose a random seed for the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "397192a1-abfb-439b-b431-50b7c84de75c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1733643247989,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "SEED = 1234\n\nenv.reset(seed=SEED);"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "env.reset(seed=SEED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "833fb94b-1ec4-4779-884c-6def0413fd9d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1733643248041,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "np.random.seed(SEED)\ntorch.manual_seed(SEED)\n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc22b771790>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3fff30-925f-4f92-9188-a592c9eedf46",
   "metadata": {},
   "source": [
    "Create the neural net for the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "760bbba5-4509-498b-a80d-cad1f3c50c6e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 56,
    "lastExecutedAt": 1733643248097,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "class PolicyNetwork(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n        super().__init__()\n\n        self.layer1 = nn.Linear(input_dim, hidden_dim)\n        self.layer2 = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        x = self.layer2(x)\n        return x\n"
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff18c400-cf69-4cac-85c6-a48e413b28a7",
   "metadata": {},
   "source": [
    "Write the function to calculate the returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dab111a-daf9-416b-b1e7-cde94823883e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1733643248149,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def calculate_stepwise_returns(rewards, discount_factor):\n    returns = []\n    R = 0\n\n    for r in reversed(rewards):\n        R = r + R * discount_factor\n        returns.insert(0, R)\n    returns = torch.tensor(returns)\n    normalized_returns = (returns - returns.mean()) / returns.std()\n    return normalized_returns\n"
   },
   "outputs": [],
   "source": [
    "def calculate_stepwise_returns(rewards, discount_factor):\n",
    "    returns = []\n",
    "    R = 0\n",
    "\n",
    "    for r in reversed(rewards):\n",
    "        R = r + R * discount_factor\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    normalized_returns = (returns - returns.mean()) / returns.std()\n",
    "    return normalized_returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fdce74-9f74-4160-b050-6f80558295e2",
   "metadata": {},
   "source": [
    "Implement the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4b6412c-0eb1-418b-9adf-01517bbfb41a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 56,
    "lastExecutedAt": 1733643248205,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def forward_pass(env, policy, discount_factor):\n    log_prob_actions = []\n    rewards = []\n    done = False\n    episode_return = 0\n\n    policy.train()\n    observation, info = env.reset()\n\n    while not done:\n        observation = torch.FloatTensor(observation).unsqueeze(0)\n        action_pred = policy(observation)\n        action_prob = F.softmax(action_pred, dim = -1)\n        dist = distributions.Categorical(action_prob)\n        action = dist.sample()\n        log_prob_action = dist.log_prob(action)\n\n        observation, reward, terminated, truncated, info = env.step(action.item())\n        done = terminated or truncated\n\n        log_prob_actions.append(log_prob_action)\n        rewards.append(reward)\n        episode_return += reward\n\n    log_prob_actions = torch.cat(log_prob_actions)\n    stepwise_returns = calculate_stepwise_returns(rewards, discount_factor)\n\n    return episode_return, stepwise_returns, log_prob_actions\n"
   },
   "outputs": [],
   "source": [
    "def forward_pass(env, policy, discount_factor):\n",
    "    log_prob_actions = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_return = 0\n",
    "\n",
    "    policy.train()\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        observation = torch.FloatTensor(observation).unsqueeze(0)\n",
    "        action_pred = policy(observation)\n",
    "        action_prob = F.softmax(action_pred, dim = -1)\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        action = dist.sample()\n",
    "        log_prob_action = dist.log_prob(action)\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        log_prob_actions.append(log_prob_action)\n",
    "        rewards.append(reward)\n",
    "        episode_return += reward\n",
    "\n",
    "    log_prob_actions = torch.cat(log_prob_actions)\n",
    "    stepwise_returns = calculate_stepwise_returns(rewards, discount_factor)\n",
    "\n",
    "    return episode_return, stepwise_returns, log_prob_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3c5a6-7fba-404a-bc6b-118edfe884bd",
   "metadata": {},
   "source": [
    "Calculate the loss function as the expected value of the returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d4c4f21-4815-4173-8338-50cf67a2f48d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1733643248257,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def calculate_loss(stepwise_returns, log_prob_actions):\n    loss = -(stepwise_returns * log_prob_actions).sum()\n    return loss\n"
   },
   "outputs": [],
   "source": [
    "def calculate_loss(stepwise_returns, log_prob_actions):\n",
    "    loss = -(stepwise_returns * log_prob_actions).sum()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2c196-d607-4eb2-b1f4-771cc26c0997",
   "metadata": {},
   "source": [
    "Use back propagation to update the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5cff342-6db0-4643-a28f-ae59803ecf5b",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 48,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1733643248305,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def update_policy(stepwise_returns, log_prob_actions, optimizer):\n    stepwise_returns = stepwise_returns.detach()\n    loss = calculate_loss(stepwise_returns, log_prob_actions)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n\n"
   },
   "outputs": [],
   "source": [
    "def update_policy(stepwise_returns, log_prob_actions, optimizer):\n",
    "    stepwise_returns = stepwise_returns.detach()\n",
    "    loss = calculate_loss(stepwise_returns, log_prob_actions)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c580e-ff07-467c-be86-34cdf8a69955",
   "metadata": {},
   "source": [
    "Declare the hyperparameters, run the training epochs, and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8241c0b-d626-488b-9bcd-4b618bb4d853",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1733643248357,
    "lastExecutedByKernel": "c44d1f8e-829d-4823-8dad-2cadde8fb3bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def main(): \n    MAX_EPOCHS = 500\n    DISCOUNT_FACTOR = 0.99\n    N_TRIALS = 25\n    REWARD_THRESHOLD = 475\n    PRINT_INTERVAL = 10\n    INPUT_DIM = env.observation_space.shape[0]\n    HIDDEN_DIM = 128\n    OUTPUT_DIM = env.action_space.n\n    DROPOUT = 0.5\n\n    episode_returns = []\n\n    policy = PolicyNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)\n\n    LEARNING_RATE = 0.01\n    optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE)\n\n    for episode in range(1, MAX_EPOCHS+1):\n        episode_return, stepwise_returns, log_prob_actions = forward_pass(env, policy, DISCOUNT_FACTOR)\n        _ = update_policy(stepwise_returns, log_prob_actions, optimizer)\n\n        episode_returns.append(episode_return)\n        mean_episode_return = np.mean(episode_returns[-N_TRIALS:])\n\n        if episode % PRINT_INTERVAL == 0:\n            print(f'| Episode: {episode:3} | Mean Rewards: {mean_episode_return:5.1f} |')\n\n        if mean_episode_return >= REWARD_THRESHOLD:\n            print(f'Reached reward threshold in {episode} episodes')\n            break\n"
   },
   "outputs": [],
   "source": [
    "def main(): \n",
    "    MAX_EPOCHS = 500\n",
    "    DISCOUNT_FACTOR = 0.99\n",
    "    N_TRIALS = 25\n",
    "    REWARD_THRESHOLD = 475\n",
    "    PRINT_INTERVAL = 10\n",
    "    INPUT_DIM = env.observation_space.shape[0]\n",
    "    HIDDEN_DIM = 128\n",
    "    OUTPUT_DIM = env.action_space.n\n",
    "    DROPOUT = 0.5\n",
    "\n",
    "    episode_returns = []\n",
    "\n",
    "    policy = PolicyNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)\n",
    "\n",
    "    LEARNING_RATE = 0.01\n",
    "    optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "    for episode in range(1, MAX_EPOCHS+1):\n",
    "        episode_return, stepwise_returns, log_prob_actions = forward_pass(env, policy, DISCOUNT_FACTOR)\n",
    "        _ = update_policy(stepwise_returns, log_prob_actions, optimizer)\n",
    "\n",
    "        episode_returns.append(episode_return)\n",
    "        mean_episode_return = np.mean(episode_returns[-N_TRIALS:])\n",
    "\n",
    "        if episode % PRINT_INTERVAL == 0:\n",
    "            print(f'| Episode: {episode:3} | Mean Rewards: {mean_episode_return:5.1f} |')\n",
    "\n",
    "        if mean_episode_return >= REWARD_THRESHOLD:\n",
    "            print(f'Reached reward threshold in {episode} episodes')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a370a3-02ca-4159-8430-3a130e14c1a5",
   "metadata": {},
   "source": [
    "Run the program. \n",
    "If the training doesn't converge, run it again, or change the SEED values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75423488-ad1f-412d-9aed-f59759329d57",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode:  10 | Mean Rewards:  24.4 |\n",
      "| Episode:  20 | Mean Rewards:  18.1 |\n",
      "| Episode:  30 | Mean Rewards:  14.4 |\n",
      "| Episode:  40 | Mean Rewards:  14.3 |\n",
      "| Episode:  50 | Mean Rewards:  23.8 |\n",
      "| Episode:  60 | Mean Rewards:  44.1 |\n",
      "| Episode:  70 | Mean Rewards:  71.1 |\n",
      "| Episode:  80 | Mean Rewards: 113.9 |\n",
      "| Episode:  90 | Mean Rewards: 129.1 |\n",
      "| Episode: 100 | Mean Rewards: 148.7 |\n",
      "| Episode: 110 | Mean Rewards: 225.4 |\n",
      "| Episode: 120 | Mean Rewards: 319.4 |\n",
      "| Episode: 130 | Mean Rewards: 359.7 |\n",
      "| Episode: 140 | Mean Rewards: 361.8 |\n",
      "| Episode: 150 | Mean Rewards: 422.7 |\n",
      "| Episode: 160 | Mean Rewards: 420.8 |\n",
      "| Episode: 170 | Mean Rewards: 279.8 |\n",
      "| Episode: 180 | Mean Rewards: 125.2 |\n",
      "| Episode: 190 | Mean Rewards: 126.1 |\n",
      "| Episode: 200 | Mean Rewards: 169.5 |\n",
      "| Episode: 210 | Mean Rewards: 240.3 |\n",
      "| Episode: 220 | Mean Rewards: 263.0 |\n",
      "| Episode: 230 | Mean Rewards: 276.6 |\n",
      "| Episode: 240 | Mean Rewards: 273.4 |\n",
      "| Episode: 250 | Mean Rewards: 309.0 |\n",
      "| Episode: 260 | Mean Rewards: 362.0 |\n",
      "| Episode: 270 | Mean Rewards: 318.0 |\n",
      "| Episode: 280 | Mean Rewards: 208.6 |\n",
      "| Episode: 290 | Mean Rewards: 130.9 |\n",
      "| Episode: 300 | Mean Rewards: 115.3 |\n",
      "| Episode: 310 | Mean Rewards: 115.1 |\n",
      "| Episode: 320 | Mean Rewards: 124.0 |\n",
      "| Episode: 330 | Mean Rewards: 124.0 |\n",
      "| Episode: 340 | Mean Rewards: 117.6 |\n",
      "| Episode: 350 | Mean Rewards: 115.6 |\n",
      "| Episode: 360 | Mean Rewards: 133.0 |\n",
      "| Episode: 370 | Mean Rewards: 186.5 |\n",
      "| Episode: 380 | Mean Rewards: 325.2 |\n",
      "| Episode: 390 | Mean Rewards: 447.1 |\n",
      "Reached reward threshold in 394 episodes\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
