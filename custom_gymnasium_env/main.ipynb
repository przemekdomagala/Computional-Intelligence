{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a176d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "\n",
    "CAR_COLORS = [(255, 0, 0)]\n",
    "\n",
    "class CrossyRoadEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.width = 40\n",
    "        self.height = 25\n",
    "        self.observation_space = spaces.Box(low=0, high=5, shape=(self.height, self.width), dtype=np.int32)\n",
    "        self.action_space = spaces.Discrete(5)  # 0: up, 1: down, 2: left, 3: right\n",
    "        \n",
    "        # Inicjalizacja PyGame do renderowania\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.width * 40, self.height * 40))\n",
    "        self.clock = pygame.time.Clock()\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.steps = 0  # Initialize step counter\n",
    "        self.max_steps = 500 \n",
    "\n",
    "        self.agent_pos = [self.width // 2, self.height - 1]\n",
    "        self.cars = []\n",
    "        self.logs = []\n",
    "        self.water_rows = set()\n",
    "        self.car_rows = set()\n",
    "        self.done = False\n",
    "        self.score = 0\n",
    "\n",
    "        reserved_rows = [0, self.height - 0, self.height - 1, self.height - 2]  # Zarezerwowane wiersze\n",
    "        all_valid_rows = [y for y in range(self.height - 1) if y not in reserved_rows]\n",
    "\n",
    "        max_water = max(1, int(0.4 * len(all_valid_rows)))\n",
    "        min_water = max(1, int(0.2 * len(all_valid_rows)))\n",
    "        water_rand = random.randint(min_water, max_water)\n",
    "        self.water_rows = set(random.sample(all_valid_rows, min(water_rand, len(all_valid_rows))))\n",
    "        \n",
    "        possible_car_rows = list(set(all_valid_rows) - self.water_rows)\n",
    "        max_road = max(1, int(0.4 * len(possible_car_rows)))\n",
    "        min_road = max(1, int(0.2 * len(possible_car_rows)))\n",
    "        road_rand = random.randint(min_road, max_road)\n",
    "        self.car_rows = set(random.sample(possible_car_rows, min(road_rand, len(possible_car_rows))))\n",
    "\n",
    "\n",
    "        # Twórz auta – 1–5 na drogę\n",
    "        for row in self.car_rows:\n",
    "            direction = random.choice([-1, 1])\n",
    "            num_cars = random.randint(1, 5)\n",
    "            for _ in range(num_cars):\n",
    "                x = random.randint(0, self.width - 1)\n",
    "                color = random.choice(CAR_COLORS)\n",
    "                self.cars.append([x, row, direction, color])\n",
    "\n",
    "\n",
    "        # Twórz kłody – po 1–3 na rzekę (długość 3)\n",
    "        prev_direction = None\n",
    "        for row in sorted(self.water_rows):\n",
    "            # Wymuś inny kierunek niż w poprzednim wierszu\n",
    "            direction = random.choice([-1, 1])\n",
    "            if prev_direction is not None:\n",
    "                direction = -prev_direction\n",
    "            prev_direction = direction\n",
    "\n",
    "            for _ in range(random.randint(1, 3)):\n",
    "                start_x = random.randint(0, self.width - 3)\n",
    "                self.logs.append([start_x, row, direction])\n",
    "                \n",
    "        # Dodaj skały – np. 20 losowych\n",
    "        self.rocks = []\n",
    "        rocks_count = np.random.randint(0.1 * self.width * self.height, 0.15 * self.width * self.height)\n",
    "        for _ in range(rocks_count):\n",
    "            attempt = 0\n",
    "            while attempt < 100:  # nie nieskończoność\n",
    "                rx = random.randint(0, self.width - 1)\n",
    "                ry = random.randint(1, self.height - 1)\n",
    "\n",
    "                if (\n",
    "                    ry in self.water_rows or\n",
    "                    ry in self.car_rows or\n",
    "                    ry >= self.height - 4 or\n",
    "                    [rx, ry] == self.agent_pos or\n",
    "                    [rx, ry] in self.rocks\n",
    "                ):\n",
    "                    attempt += 1\n",
    "                    continue\n",
    "                \n",
    "                self.rocks.append([rx, ry])\n",
    "                break\n",
    "\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self._get_obs(), 0, True, False, {}\n",
    "        \n",
    "        self.steps += 1\n",
    "        if self.steps >= self.max_steps:\n",
    "            return self._get_obs(), -15.0, True, False, {\"TimeLimit.truncated\": True}\n",
    "    \n",
    "        reward = 0.0\n",
    "        old_x, old_y = self.agent_pos\n",
    "    \n",
    "        # Movement logic (unchanged)\n",
    "        x, y = self.agent_pos\n",
    "        if action == 0 and y > 0: y -= 1\n",
    "        elif action == 1 and y < self.height - 1: y += 1\n",
    "        elif action == 2 and x > 0: x -= 1\n",
    "        elif action == 3 and x < self.width - 1: x += 1\n",
    "        elif action == 4: pass  # no-op\n",
    "        \n",
    "        self.agent_pos = [x, y]\n",
    "    \n",
    "        # ---- New Reward Shaping ----\n",
    "        # Big reward for upward movement\n",
    "        if y < old_y: reward += 4.0\n",
    "        \n",
    "        # Penalize hesitation\n",
    "        if self.agent_pos == [old_x, old_y]: reward -= 2.0\n",
    "        \n",
    "        # Small penalty for moving down\n",
    "        elif y > old_y: reward -= 0.5\n",
    "        \n",
    "        # Time bonus (faster = better)\n",
    "        reward += 0.1 * (self.max_steps - self.steps)\n",
    "    \n",
    "        # ---- Updated Failure Penalties ----\n",
    "        # Collision with car (less severe)\n",
    "        for car in self.cars:\n",
    "            if self.agent_pos == [car[0], car[1]]:\n",
    "                return self._get_obs(), -5.0, True, False, {}\n",
    "        \n",
    "        # Drowning (less severe)\n",
    "        if self.agent_pos[1] in self.water_rows:\n",
    "            on_log = False\n",
    "            for log in self.logs:\n",
    "                if log[1] == self.agent_pos[1] and log[0] <= self.agent_pos[0] < log[0] + 3:\n",
    "                    on_log = True\n",
    "                    break\n",
    "            if not on_log:\n",
    "                return self._get_obs(), -5.0, True, False, {}\n",
    "    \n",
    "        # Goal (unchanged, still awesome)\n",
    "        if self.agent_pos[1] == 0:\n",
    "            reward += 50.0\n",
    "            return self._get_obs(), reward, True, False, {}\n",
    "    \n",
    "        return self._get_obs(), reward, False, False, {}\n",
    "\n",
    "\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        grid = np.zeros((self.height, self.width), dtype=np.int32)\n",
    "\n",
    "        # Pola drogi\n",
    "        for y in self.car_rows:\n",
    "            for x in range(self.width):\n",
    "                grid[y, x] = 5\n",
    "\n",
    "        grid[self.agent_pos[1], self.agent_pos[0]] = 1  # Agent\n",
    "\n",
    "        for car in self.cars:\n",
    "            if 0 <= car[1] < self.height and 0 <= car[0] < self.width:\n",
    "                grid[car[1], car[0]] = 2\n",
    "\n",
    "        for log in self.logs:\n",
    "            for i in range(3):\n",
    "                lx = log[0] + i\n",
    "                if 0 <= lx < self.width:\n",
    "                    grid[log[1], lx] = 3\n",
    "\n",
    "        # Zaznacz wodę tam, gdzie nie ma kłody\n",
    "        for y in self.water_rows:\n",
    "            for x in range(self.width):\n",
    "                if grid[y, x] == 0:\n",
    "                    grid[y, x] = 4\n",
    "\n",
    "        # Skały jako np. wartość 5 w macierzy\n",
    "        for rock in self.rocks:\n",
    "            grid[rock[1], rock[0]] = 5\n",
    "\n",
    "\n",
    "        return grid\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                exit()\n",
    "\n",
    "\n",
    "        self.screen.fill((170, 220 ,60))\n",
    "        \n",
    "        # Rysuj tło – puste pola\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                pygame.draw.rect(self.screen, (125, 235, 80), (x * 40, y * 40, 40, 40))  # jasna zieleń\n",
    "\n",
    "        # Rysuj wodę\n",
    "        for y in self.water_rows:\n",
    "            for x in range(self.width):\n",
    "                if all(not (log[1] == y and log[0] <= x < log[0] + 3) for log in self.logs):\n",
    "                    pygame.draw.rect(self.screen, (0, 180, 215), (x * 40, y * 40, 40, 40))\n",
    "\n",
    "        \n",
    "        # Rysuj drogę\n",
    "        for y in self.car_rows:\n",
    "            for x in range(self.width):\n",
    "                pygame.draw.rect(self.screen, (160, 130, 100), (x * 40, y * 40, 40, 40))  # ciemnoszary asfalt\n",
    "        \n",
    "        # Rysuj samochody\n",
    "        for car in self.cars:\n",
    "            pygame.draw.rect(self.screen, (255, 55, 55), (car[0] * 40, car[1] * 40, 40, 40))\n",
    "\n",
    "        for log in self.logs:\n",
    "            for i in range(3):\n",
    "                lx = log[0] + i\n",
    "                if 0 <= lx < self.width:\n",
    "                    pygame.draw.rect(self.screen, (145, 65, 30), (lx * 40, log[1] * 40, 40, 40))\n",
    "\n",
    "        # Rysuj skały\n",
    "        for rock in self.rocks:\n",
    "            pygame.draw.rect(self.screen, (100, 100, 100), (rock[0] * 40, rock[1] * 40, 40, 40))\n",
    "\n",
    "        \n",
    "        pygame.draw.circle(self.screen, (255, 200, 0), (self.agent_pos[0] * 40 + 20, self.agent_pos[1] * 40 + 20), 20)\n",
    "        \n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(12)  # 10 FPS\n",
    "    \n",
    "    def close(self):\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba07346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, num_values=6):\n",
    "        super().__init__(env)\n",
    "        self.num_values = num_values\n",
    "        height, width = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0,\n",
    "            high=1.0,\n",
    "            shape=(height, width, num_values),  # (H, W, C)\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        one_hot = np.eye(self.num_values)[obs]  # (H, W, C)\n",
    "        return one_hot.astype(np.float32)\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=128):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        with th.no_grad():\n",
    "            sample_input = th.as_tensor(observation_space.sample()[None]).float()\n",
    "            n_flatten = self.cnn(sample_input).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations):\n",
    "        return self.linear(self.cnn(observations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4795bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Eval num_timesteps=4000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 4.89e-06 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 7        |\n",
      "|    policy_loss        | 1.5e+03  |\n",
      "|    value_loss         | 1.58e+06 |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 3.81e-06 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 15       |\n",
      "|    policy_loss        | 1.36e+03 |\n",
      "|    value_loss         | 1.03e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 8.34e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 23       |\n",
      "|    policy_loss        | 1.63e+03 |\n",
      "|    value_loss         | 1.72e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 5.36e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 31       |\n",
      "|    policy_loss        | 1.35e+03 |\n",
      "|    value_loss         | 1.12e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.49     |\n",
      "|    explained_variance | -4.77e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 39        |\n",
      "|    policy_loss        | 1.54e+03  |\n",
      "|    value_loss         | 1.77e+06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 4.77e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 46       |\n",
      "|    policy_loss        | 1.74e+03 |\n",
      "|    value_loss         | 1.98e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | -5.96e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 54        |\n",
      "|    policy_loss        | 1.43e+03  |\n",
      "|    value_loss         | 1.6e+06   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | -3.58e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 62        |\n",
      "|    policy_loss        | 1.52e+03  |\n",
      "|    value_loss         | 1.76e+06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 70       |\n",
      "|    policy_loss        | 1.04e+03 |\n",
      "|    value_loss         | 9.33e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | -7.15e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 78        |\n",
      "|    policy_loss        | 761       |\n",
      "|    value_loss         | 6.75e+05  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 85       |\n",
      "|    policy_loss        | 1.2e+03  |\n",
      "|    value_loss         | 9.51e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 93        |\n",
      "|    policy_loss        | 1.4e+03   |\n",
      "|    value_loss         | 1.56e+06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 292      |\n",
      "|    ep_rew_mean        | 8.07e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 1643     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 51200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 2.98e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 888      |\n",
      "|    value_loss         | 7.41e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 52000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.4      |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 101       |\n",
      "|    policy_loss        | 2.03e+03  |\n",
      "|    value_loss         | 2.66e+06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 56000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | -8.34e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 109       |\n",
      "|    policy_loss        | 801       |\n",
      "|    value_loss         | 5.58e+05  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 117      |\n",
      "|    policy_loss        | 764      |\n",
      "|    value_loss         | 5.02e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 64000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 124       |\n",
      "|    policy_loss        | 1.64e+03  |\n",
      "|    value_loss         | 1.65e+06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 68000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 132       |\n",
      "|    policy_loss        | 1.37e+03  |\n",
      "|    value_loss         | 1.62e+06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 72000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 140       |\n",
      "|    policy_loss        | 989       |\n",
      "|    value_loss         | 7.7e+05   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 148      |\n",
      "|    policy_loss        | 1.29e+03 |\n",
      "|    value_loss         | 1.67e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 80000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.31     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 156       |\n",
      "|    policy_loss        | 1.19e+03  |\n",
      "|    value_loss         | 1.4e+06   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 84000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 164      |\n",
      "|    policy_loss        | 1.29e+03 |\n",
      "|    value_loss         | 1.84e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 88000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.32     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 171       |\n",
      "|    policy_loss        | 1.18e+03  |\n",
      "|    value_loss         | 1.15e+06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 179      |\n",
      "|    policy_loss        | 784      |\n",
      "|    value_loss         | 5.53e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 96000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.3      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 187       |\n",
      "|    policy_loss        | 657       |\n",
      "|    value_loss         | 5.37e+05  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.32     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 195       |\n",
      "|    policy_loss        | 699       |\n",
      "|    value_loss         | 4.75e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 359       |\n",
      "|    ep_rew_mean        | 9.33e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 1618      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 102400    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 711       |\n",
      "|    value_loss         | 5.12e+05  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 104000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 203      |\n",
      "|    policy_loss        | 810      |\n",
      "|    value_loss         | 6.99e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 108000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 210      |\n",
      "|    policy_loss        | 754      |\n",
      "|    value_loss         | 5.09e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 112000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 218      |\n",
      "|    policy_loss        | 712      |\n",
      "|    value_loss         | 3.67e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 116000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 226      |\n",
      "|    policy_loss        | 921      |\n",
      "|    value_loss         | 6.54e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 120000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 234      |\n",
      "|    policy_loss        | 1.25e+03 |\n",
      "|    value_loss         | 1.44e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 124000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 242      |\n",
      "|    policy_loss        | 386      |\n",
      "|    value_loss         | 1.7e+05  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 128000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 249      |\n",
      "|    policy_loss        | 1.34e+03 |\n",
      "|    value_loss         | 1.51e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 132000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 257      |\n",
      "|    policy_loss        | 850      |\n",
      "|    value_loss         | 4.95e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 136000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 265      |\n",
      "|    policy_loss        | 817      |\n",
      "|    value_loss         | 5.41e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 140000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 273      |\n",
      "|    policy_loss        | 703      |\n",
      "|    value_loss         | 4.24e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 144000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 281      |\n",
      "|    policy_loss        | 503      |\n",
      "|    value_loss         | 2.72e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 148000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 289      |\n",
      "|    policy_loss        | 788      |\n",
      "|    value_loss         | 9.42e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 152000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 296      |\n",
      "|    policy_loss        | 1.71e+03 |\n",
      "|    value_loss         | 2.02e+06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 429      |\n",
      "|    ep_rew_mean        | 1.07e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 1623     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 94       |\n",
      "|    total_timesteps    | 153600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.58e+03 |\n",
      "|    value_loss         | 1.67e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 156000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 304       |\n",
      "|    policy_loss        | 928       |\n",
      "|    value_loss         | 6.21e+05  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 160000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 312      |\n",
      "|    policy_loss        | 948      |\n",
      "|    value_loss         | 1.02e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 164000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 320       |\n",
      "|    policy_loss        | 1.05e+03  |\n",
      "|    value_loss         | 1.23e+06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 168000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 328      |\n",
      "|    policy_loss        | 393      |\n",
      "|    value_loss         | 1.42e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 172000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 335      |\n",
      "|    policy_loss        | 1.15e+03 |\n",
      "|    value_loss         | 1.03e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 176000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 343      |\n",
      "|    policy_loss        | 1.12e+03 |\n",
      "|    value_loss         | 1.05e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 180000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 351      |\n",
      "|    policy_loss        | 1.31e+03 |\n",
      "|    value_loss         | 1.39e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 184000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 359       |\n",
      "|    policy_loss        | 881       |\n",
      "|    value_loss         | 8.67e+05  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | 1.15e+04  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 188000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 367       |\n",
      "|    policy_loss        | 315       |\n",
      "|    value_loss         | 9.22e+04  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 192000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 374      |\n",
      "|    policy_loss        | 944      |\n",
      "|    value_loss         | 6.8e+05  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 196000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 382      |\n",
      "|    policy_loss        | 957      |\n",
      "|    value_loss         | 7.63e+05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=11462.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 1.15e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 200000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 390      |\n",
      "|    policy_loss        | 660      |\n",
      "|    value_loss         | 3.31e+05 |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "env = make_vec_env(lambda: OneHotWrapper(CrossyRoadEnv()), n_envs=4)\n",
    "\n",
    "model = A2C(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=0.0003,\n",
    "    n_steps=128,\n",
    "    gamma=0.99,\n",
    "    ent_coef=0.01,\n",
    "    policy_kwargs=dict(\n",
    "        normalize_images=False,\n",
    "        features_extractor_class=CustomCNN,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Callback do oceny postępów\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    eval_freq=1000,\n",
    "    best_model_save_path=\"./a2c_crossyroad/\",\n",
    ")\n",
    "\n",
    "# Trening!\n",
    "model.learn(total_timesteps=200_000, callback=eval_callback)\n",
    "\n",
    "# Zapisz model\n",
    "model.save(\"a2c_crossyroad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c609bb71",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[1;32m     10\u001b[0m obs, _, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 11\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     13\u001b[0m     obs, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/studia/inteligencja_obliczeniowa/projekt4/.venv/lib/python3.10/site-packages/gymnasium/core.py:337\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 240\u001b[0m, in \u001b[0;36mCrossyRoadEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mcircle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m20\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m20\u001b[39m), \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m    239\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ładowanie modelu\n",
    "model = A2C.load(\"a2c_crossyroad/best_model.zip\")\n",
    "\n",
    "\n",
    "env = OneHotWrapper(CrossyRoadEnv())  # Użyj wrappera, tak jak przy trenowaniu\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, done, _, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f7074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     env = CrossyRoadEnv()\n",
    "#     obs, _ = env.reset()\n",
    "#     for _ in range(100):\n",
    "#         action = env.action_space.sample()\n",
    "#         obs, reward, done, _, _ = env.step(action)\n",
    "#         env.render()\n",
    "#         if done:\n",
    "#             obs, _ = env.reset()\n",
    "#     env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1ab4f",
   "metadata": {},
   "source": [
    "# Uczenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ae71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "# from stable_baselines3 import DQN\n",
    "\n",
    "# env = make_vec_env(lambda: CrossyRoadEnv(), n_envs=1)\n",
    "# env = VecTransposeImage(env)  # (C, H, W) for CNN\n",
    "\n",
    "# # Zbuduj model DQN\n",
    "# model = DQN(\n",
    "#     policy=\"CnnPolicy\",\n",
    "#     env=env,\n",
    "#     exploration_fraction=0.3,\n",
    "#     exploration_final_eps=0.05,\n",
    "#     learning_rate=1e-4,\n",
    "#     buffer_size=50000,\n",
    "#     learning_starts=1000,\n",
    "#     batch_size=64,\n",
    "#     tau=1.0,\n",
    "#     gamma=0.99,\n",
    "#     train_freq=4,\n",
    "#     target_update_interval=1000,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Trenuj agenta\n",
    "# model.learn(total_timesteps=100_000)\n",
    "\n",
    "# # Zapisz model\n",
    "# model.save(\"dqn_crossy_model\")\n",
    "\n",
    "# print(\"Trening zakończony. Model zapisany jako 'dqn_crossy_model'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce11e4",
   "metadata": {},
   "source": [
    "## Uruchomienie po learningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ed4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from stable_baselines3 import DQN\n",
    "# from gymnasium.wrappers import FlattenObservation\n",
    "\n",
    "# env = CrossyRoadEnv()\n",
    "# env = FlattenObservation(env)\n",
    "\n",
    "# model = DQN.load(\"dqn_crossy_model\")\n",
    "\n",
    "# obs, _ = env.reset()\n",
    "# start_time = time.time()\n",
    "\n",
    "# while True:\n",
    "#     action, _ = model.predict(obs)\n",
    "#     obs, reward, done, _, _ = env.step(action)\n",
    "#     env.render()\n",
    "\n",
    "#     # Sprawdzenie czasu\n",
    "#     if time.time() - start_time > 60:\n",
    "#         print(\"Koniec po 60 sekundach.\")\n",
    "#         break\n",
    "\n",
    "#     if done:\n",
    "#         obs, _ = env.reset()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
